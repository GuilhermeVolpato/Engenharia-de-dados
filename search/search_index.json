{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documenta\u00e7\u00e3o do Sistema Pingado Caf\u00e9","text":""},{"location":"#introducao-ao-projeto-pingado-cafe","title":"Introdu\u00e7\u00e3o ao Projeto Pingado Caf\u00e9","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto Pingado Caf\u00e9, um sistema de gerenciamento desenvolvido para a mat\u00e9ria de Engenharia de Dados, utilizando como base um banco relacional de um projeto passado, para alimentar uma tabela Data Lake, e apresentar um Dashboard com resultados da Cafeteria.</p>"},{"location":"#visao-geral","title":"Vis\u00e3o Geral","text":"<ul> <li>O ambiente relacional \u2013 origem \u2013 tem 6 tabelas, 10.000 linhas para cada tabela principal e com distribui\u00e7\u00e3o de datas para os \u00faltimos 3 anos ( O banco de dados utilizado foi modelado na mat\u00e9ria de Banco de Dados 2, Modelo Relacional).</li> <li>Foi utilizado a biblioteca Faker do Python, para gerar as massas de dados e popular o ambiente relacional.</li> <li>A ingest\u00e3o dos dados foi feita atrav\u00e9s do Azure DataBricks (cloud).</li> <li>O Data Lake foi criado em cima de um object storage (cloud) usando a arquitetura medalh\u00e3o (camadas Landing, Bronze, Silver e Gold).</li> <li>Os dados ser\u00e3o gravados no object storage no formato Delta Lake nas camadas Bronze, Silver e Gold.   A transforma\u00e7\u00e3o ser\u00e1 feita atrav\u00e9s do Apache Spark (Python/pyspark).</li> <li>As fun\u00e7\u00f5es de ingest\u00e3o, transforma\u00e7\u00e3o e movimenta\u00e7\u00e3o dos dados entre as camadas s\u00e3o   orquestradas e agendadas atrav\u00e9s da ferramenta Azure DataBricks.</li> <li>Os dados ser\u00e3o disponibilizados na camada Gold no formato dimensional (OBT).</li> <li>Foram criadas 2 KPIs e 4 m\u00e9tricas para compor o dashboard no DataBricks.</li> <li>O dashboard consome os dados do modelo OBT, direto da camada gold.</li> <li>A documenta\u00e7\u00e3o completa do trabalho est\u00e1 publicada nesse MkDocs.</li> </ul>"},{"location":"#objetivo-do-projeto","title":"Objetivo do Projeto","text":"<p>O objetivo do projeto Pingado Caf\u00e9 \u00e9 desenvolver um sistema de gerenciamento de dados que utilize um banco de dados relacional existente para alimentar um Data Lake, possibilitando a cria\u00e7\u00e3o de um Dashboard para a apresenta\u00e7\u00e3o de resultados sobre a opera\u00e7\u00e3o da cafeteria. Este projeto visa a integra\u00e7\u00e3o e transforma\u00e7\u00e3o de grandes volumes de dados, aplicando uma arquitetura moderna e eficiente para armazenamento e processamento de dados, com o uso de tecnologias (Azure DataBricks, Delta Lake e Apache Spark). Com isso, visando criar uma dashboard para an\u00e1lise de dados, permitindo uma vis\u00e3o detalhada e otimizada do desempenho da cafeteria, atrav\u00e9s de KPIs e m\u00e9tricas espec\u00edficas, e a disponibiliza\u00e7\u00e3o desses dados em um formato dimensional (OBT) adequado para facil observa\u00e7\u00e3o dos dados.</p>"},{"location":"#desenho-de-arquitetura","title":"Desenho de Arquitetura:","text":""},{"location":"#estrutura-da-documentacao","title":"Estrutura da Documenta\u00e7\u00e3o","text":"<p>Esta documenta\u00e7\u00e3o foi organizada para guiar voc\u00ea por todas as funcionalidades e componentes do Pingado Caf\u00e9. Aqui, voc\u00ea encontrar\u00e1:</p> <ul> <li>-&gt; Apresenta\u00e7\u00e3o.</li> <li>-&gt; Pr\u00e9-Requisitos e Ferramentas.</li> <li>-&gt; Execu\u00e7\u00e3o.</li> </ul>"},{"location":"#integrantes","title":"Integrantes","text":"<ul> <li>Charles Clezar</li> <li>Gabriel Canarin Salazar</li> <li>Guilherme Silveira</li> <li>Guilherme Volpato</li> <li>Jo\u00e3o Eduardo Milak Farias</li> <li>Luiz Otavio Vieira</li> <li>Naum Marcirio</li> <li>Pedro Hahn</li> </ul>"},{"location":"comoExecutar/","title":"Como Executar","text":""},{"location":"comoExecutar/#scripts-que-serao-executados","title":"Scripts que ser\u00e3o executados:","text":""},{"location":"comoExecutar/#escrevendo-dados-na-camada-landing","title":"Escrevendo dados na camada landing.","text":"<p><pre><code>spark\n\n\n\nHost = \"\"\nPort = 1433\nDatabase = \"pingado-database\"\nUsername = \"\"\nPassword = \"\"\nDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nUrl = f\"jdbc:sqlserver://{Host}:{Port};databaseName={Database}\"\n\n\n\ndf_cardapio = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"cardapio\").option(\"user\", Username).option(\"password\", Password).load()\ndf_comanda = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"comanda\").option(\"user\", Username).option(\"password\", Password).load() \ndf_estoque = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"estoque\").option(\"user\", Username).option(\"password\", Password).load() \ndf_funcionarios = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"funcionarios\").option(\"user\", Username).option(\"password\", Password).load() \ndf_ingredientes = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"ingredientes\").option(\"user\", Username).option(\"password\", Password).load()\ndf_mesas = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"mesas\").option(\"user\", Username).option(\"password\", Password).load()\ndf_pagamento = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"pagamento\").option(\"user\", Username).option(\"password\", Password).load()\ndf_pedido = spark.read.format(\"jdbc\").option(\"driver\", Driver).option(\"url\", Url).option(\"dbtable\", \"pedido\").option(\"user\", Username).option(\"password\", Password).load()\n\n\n\ndisplay(dbutils.fs.mounts())\n\n\n\nstorageAccountName = \"datalakec9a8951eabdc0653\"\nstorageAccountAccessKey = \"\"\nsasToken = \"\"\n\ndef mount_adls(blobContainerName):\n    try:\n      dbutils.fs.mount(\n        source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n        mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n        #extra_configs = {'fs.azure.account.key.' + storageAccountName + '.blob.core.windows.net': storageAccountAccessKey}\n        extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n      )\n      print(\"OK!\")\n    except Exception as e:\n      print(\"Falha\", e)\n\n\n\ntry:\n    dbutils.fs.unmount((f\"/mnt/{storageAccountName}/landing-zone\"))\n    dbutils.fs.unmount((f\"/mnt/{storageAccountName}/bronze\"))\n    dbutils.fs.unmount((f\"/mnt/{storageAccountName}/silver\"))\n    dbutils.fs.unmount((f\"/mnt/{storageAccountName}/gold\"))\nexcept:\n    print(\"Sem pontos montados.\")\n\n\n\nmount_adls('landing-zone')\nmount_adls('bronze')\nmount_adls('silver')\nmount_adls('gold')\n\n\n\ndisplay(dbutils.fs.mounts())\n\n\n\ndf_cardapio.show(10)\ndf_comanda.show(10)\ndf_estoque.show(10)\ndf_funcionarios.show(10)\ndf_ingredientes.show(10)\ndf_mesas.show(10)\ndf_pagamento.show(10)\ndf_pedido.show(10)\n\n\n\ndf_cardapio.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Cardapio\")\ndf_comanda.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Comanda\")\ndf_estoque.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Estoque\")\ndf_funcionarios.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Funcionarios\")\ndf_ingredientes.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Ingredientes\")\ndf_mesas.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Mesas\")\ndf_pagamento.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Pagamento\")\ndf_pedido.write.option(\"header\", \"true\").format('csv').save(f\"/mnt/{storageAccountName}/landing-zone/Pedido\")\n\n\n\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Cardapio\"))\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Comanda\"))\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Estoque\"))\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Funcionarios\"))\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Ingredientes\"))\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Mesas\"))\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Pagamento\"))\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone/Pedido\"))\n</code></pre> </p>"},{"location":"comoExecutar/#passando-os-dados-para-a-camada-bronze","title":"Passando os dados para a camada bronze.","text":"<pre><code>spark\n\n\n\ndisplay(dbutils.fs.mounts())\n\n\nstorageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken = \"\"\n\n\n\ndisplay(dbutils.fs.mounts())\n\n\n\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone\"))\n\n\n\ndf_cardapio = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Cardapio\")\ndf_comanda = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Comanda\")\ndf_estoque = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Estoque\")\ndf_funcionarios = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Funcionarios\")\ndf_ingredientes = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Ingredientes\")\ndf_mesas = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Mesas\")\ndf_pagamento = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Pagamento\")\ndf_pedido = spark.read.format('csv').option(\"header\", \"true\").load(f\"/mnt/{storageAccountName}/landing-zone/Pedido\")\n\n\n\nfrom pyspark.sql.functions import current_timestamp, lit\n\ndf_cardapio = df_cardapio.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"cardapio.csv\"))\ndf_comanda = df_comanda.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"comanda.csv\"))\ndf_estoque = df_estoque.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"estoque.csv\"))\ndf_funcionarios = df_funcionarios.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"funcionarios.csv\"))\ndf_ingredientes = df_ingredientes.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"ingredientes.csv\"))\ndf_mesas = df_mesas.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"mesas.csv\"))\ndf_pagamento = df_pagamento.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"pagamento.csv\"))\ndf_pedido = df_pedido.withColumn(\"DATA_HORA_BRONZE\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"pedido.csv\"))\n\n\n\ndf_cardapio.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Cardapio\")\ndf_comanda.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Comanda\")\ndf_estoque.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Estoque\")\ndf_funcionarios.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Funcionarios\")\ndf_ingredientes.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Ingredientes\")\ndf_mesas.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Mesas\")\ndf_pagamento.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Pagamento\")\ndf_pedido.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/Pedido\")\n\n\n\nspark.read.format('delta').load(f'/mnt/{storageAccountName}/bronze/Cardapio').limit(10).display()\n</code></pre>"},{"location":"comoExecutar/#passando-os-dados-para-a-camada-prata","title":"Passando os dados para a camada prata.","text":"<pre><code>spark\n\n\n\ndisplay(dbutils.fs.mounts())\n\n\n\nstorageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken = \"\"\n\n\n\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/bronze\"))\n\n\n\ndf_cardapio = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/Cardapio\")\n\n\n\nfrom pyspark.sql.functions import current_timestamp, lit\n\ndf_cardapio = df_cardapio.withColumn(\"DATA_HORA_SILVER\", current_timestamp()).withColumn(\"NOME_ARQUIVO\", lit(\"cardapio\"))\n\n\n\ncolunas = df_cardapio.columns\n\n\ncolunas_maiusculas = [coluna.upper() for coluna in colunas]\n\n\nprint(\"Colunas em mai\u00fasculas:\")\nfor coluna in colunas_maiusculas:\n    print(coluna)\n\n\n\ndf_cardapio = (df_cardapio\n               .withColumnRenamed(\"id_item_cardapio\",\"CODIGO_ITEM_CARDAPIO\")\n               .withColumnRenamed(\"nome_item\" , \"NOME_ITEM_CARDAPIO\")\n               .withColumnRenamed(\"valor\" , \"VALOR\")\n               .withColumnRenamed(\"descricao\" , \"DESCRICAO\")\n               .withColumnRenamed(\"categoria\" , \"CATEGORIA\")\n               .withColumnRenamed(\"disponibilidade\" , \"DISPONIBILIDADE\")\n               .withColumnRenamed(\"data_hora_bronze\" , \"DATA_HORA_BRONZE\")\n               .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\")\n               .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\"))\n\n\n\ndf_cardapio.display()\n\n\n\ndf_cardapio = df_cardapio.dropDuplicates()\n\n\n\ndf_cardapio = df_cardapio.fillna({\"VALOR\": 0, \"DISPONIBILIDADE\": \"False\"})\n\n\n\ndf_cardapio.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/Cardapio\")\n\n\n\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/silver/\"))\n\n\nspark.read.format('delta').load(f'/mnt/{storageAccountName}/silver/Cardapio').limit(10).display()\n</code></pre>"},{"location":"comoExecutar/#passando-os-dados-para-a-camada-gold","title":"Passando os dados para a camada gold.","text":"<pre><code>spark\n\n\n\ndisplay(dbutils.fs.mounts())\n\n\n\nstorageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken = \"s\"\n\n\n\ndisplay(dbutils.fs.ls(f\"/mnt/{storageAccountName}/silver\"))\n\n\n\ndf_cardapio = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Cardapio\")\ndf_comanda = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Comanda\")\ndf_estoque = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Estoque\")\ndf_funcionarios = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Funcionarios\")\ndf_ingredientes = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Ingredientes\")\ndf_mesas = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Mesas\")\ndf_pagamento = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Pagamento\")\ndf_pedido = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/Pedido\")\n\n\n\ndf_obt = df_comanda \\\n    .join(df_pagamento, df_comanda.CODIGO_PAGAMENTO == df_pagamento.CODIGO_PAGAMENTO) \\\n    .join(df_funcionarios, df_comanda.CODIGO_FUNCIONARIO == df_funcionarios.CODIGO_FUNCIONARIO) \\\n    .join(df_mesas, df_comanda.CODIGO_MESA == df_mesas.CODIGO_MESA) \\\n    .join(df_pedido, df_comanda.CODIGO_COMANDA == df_pedido.CODIGO_COMANDA) \\\n    .join(df_cardapio, df_pedido.CODIGO_CARDAPIO == df_cardapio.CODIGO_ITEM_CARDAPIO) \\\n    .select(\n        df_comanda.CODIGO_COMANDA.alias(\"CODIGO_COMANDA\"),\n        df_comanda.CODIGO_MESA.alias(\"CODIGO_MESA\"),\n        df_comanda.CODIGO_PAGAMENTO.alias(\"CODIGO_PAGAMENTO\"),\n        df_comanda.CODIGO_FUNCIONARIO.alias(\"CODIGO_FUNCIONARIO\"),\n        df_comanda.VALOR_TOTAL.alias(\"VALOR_TOTAL\"),\n        df_pagamento.TIPO.alias(\"TIPO_PAGAMENTO\"),\n        df_pagamento.STATUS.alias(\"STATUS_PAGAMENTO\"),\n        df_pagamento.TAXA.alias(\"TAXA_PAGAMENTO\"),\n        df_funcionarios.NOME.alias(\"NOME_FUNCIONARIO\"),\n        df_funcionarios.SOBRENOME.alias(\"SOBRENOME_FUNCIONARIO\"),\n        df_funcionarios.TELEFONE.alias(\"TELEFONE_FUNCIONARIO\"),\n        df_funcionarios.EMAIL.alias(\"EMAIL_FUNCIONARIO\"),\n        df_funcionarios.CARGO.alias(\"CARGO_FUNCIONARIO\"),\n        df_funcionarios.DATA_CONTRATACAO.alias(\"DATA_CONTRATACAO_FUNCIONARIO\"),\n        df_mesas.QUANTIDADE_LUGARES.alias(\"QUANTIDADE_LUGARES_MESA\"),\n        df_mesas.LOCAL.alias(\"LOCAL_MESA\"),\n        df_mesas.STATUS.alias(\"STATUS_MESA\"),\n        df_pedido.CODIGO_PEDIDO.alias(\"CODIGO_PEDIDO\"),\n        df_pedido.CODIGO_CARDAPIO.alias(\"CODIGO_CARDAPIO\"),\n        df_pedido.STATUS.alias(\"STATUS_PEDIDO\"),\n        df_pedido.DATA_HORA_PEDIDO.alias(\"DATA_HORA_PEDIDO\"),\n        df_pedido.QUANTIDADE.alias(\"QUANTIDADE_PEDIDO\"),\n        df_cardapio.NOME_ITEM_CARDAPIO.alias(\"NOME_ITEM_CARDAPIO\"),\n        df_cardapio.VALOR.alias(\"VALOR_ITEM_CARDAPIO\"),\n        df_cardapio.DESCRICAO.alias(\"DESCRICAO_ITEM_CARDAPIO\"),\n        df_cardapio.CATEGORIA.alias(\"CATEGORIA_ITEM_CARDAPIO\"),\n        df_cardapio.DISPONIBILIDADE.alias(\"DISPONIBILIDADE_ITEM_CARDAPIO\"),\n        df_comanda.DATA_HORA_BRONZE.alias(\"DATA_HORA_BRONZE\"),\n        df_comanda.DATA_HORA_SILVER.alias(\"DATA_HORA_SILVER\"),\n    )\n\n\n\ndf_obt.write.format('delta').mode('overwrite').save(f'/mnt/{storageAccountName}/gold/obt_cafeteria')\n\n\n\nspark.read.format('delta').load(f'/mnt/{storageAccountName}/gold/obt_cafeteria').limit(10).display()\n</code></pre>"},{"location":"comoExecutar/#criar-as-tabelas-para-o-dashboard","title":"Criar as tabelas para o dashboard.","text":"<pre><code>spark\n\nstorageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken = \"\"\n%pip install matplotlib pandas\n\nobt = spark.read.format('delta').load(f'/mnt/{storageAccountName}/gold/obt_cafeteria')\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, year, month\nfrom datetime import datetime, timedelta\n\nspark = SparkSession.builder.appName(\"FaturamentoItem\").getOrCreate()\n\ndata_inicio = datetime.now().replace(day=1) - timedelta(days=1)\ndata_fim = datetime(data_inicio.year, data_inicio.month, 1)\n\nobt_ultimo_mes = obt.filter((col(\"data_hora_pedido\") &gt;= data_fim)\n                            &amp; (col(\"data_hora_pedido\") &lt;= data_inicio))\n\nfaturamento_por_item = obt_ultimo_mes.groupBy(\"nome_item_cardapio\") \\\n                                     .agg(sum(\"valor_item_cardapio\").alias(\"faturamento_total\")) \\\n                                     .orderBy(\"faturamento_total\", ascending=False) \\\n                                     .toPandas()\n\nnomes_itens = faturamento_por_item[\"nome_item_cardapio\"]\nfaturamentos = faturamento_por_item[\"faturamento_total\"]\n\nplt.figure(figsize=(10, 6))\nplt.bar(nomes_itens, faturamentos, color='skyblue')\nplt.xlabel('Nome do Item')\nplt.ylabel('Faturamento Total')\nplt.title('Faturamento Total por Item - \u00daltimo M\u00eas')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\nitem_maior_faturamento = faturamento_por_item.iloc[0]\nplt.annotate(f'Maior Faturamento: R$ {item_maior_faturamento[\"faturamento_total\"]:.2f}',\n             xy=(item_maior_faturamento.name, item_maior_faturamento[\"faturamento_total\"]),\n             xytext=(0, 20),\n             textcoords=\"offset points\",\n             ha='center',\n             fontsize=10,\n             bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n\nplt.show()\n\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, year, month\nfrom datetime import datetime, timedelta\n\nspark = SparkSession.builder.appName(\"KPI_Forma_Pagamento_Ultimo_Mes\").getOrCreate()\n\ndata_inicio = datetime.now().replace(day=1) - timedelta(days=1)\ndata_fim = datetime(data_inicio.year, data_inicio.month, 1)\n\nobt_ultimo_mes = obt.filter((col(\"data_hora_pedido\") &gt;= data_fim)\n                            &amp; (col(\"data_hora_pedido\") &lt;= data_inicio))\n\nfaturamento_por_forma_pagamento = obt_ultimo_mes.groupBy(\"tipo_pagamento\") \\\n                                                .agg(sum(\"valor_total\").alias(\"faturamento_total\")) \\\n                                                .orderBy(\"faturamento_total\", ascending=False) \\\n                                                .toPandas()  # Converte para Pandas para facilitar a plotagem com matplotlib\n\ntipos_pagamento = faturamento_por_forma_pagamento[\"tipo_pagamento\"]\nfaturamentos = faturamento_por_forma_pagamento[\"faturamento_total\"]\n\nplt.figure(figsize=(4, 4))\nplt.pie(faturamentos, labels=tipos_pagamento, autopct='%1.1f%%', startangle=140)\nplt.title('Faturamento por Tipo de Pagamento - \u00daltimo M\u00eas')\nplt.axis('equal')  # Mant\u00e9m o aspecto de um c\u00edrculo\nplt.tight_layout()\n\n\nplt.show()\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName(\"KPI_PedidosEmAberto\").getOrCreate()\n\npedidos_em_aberto = obt.filter(col(\"status_pagamento\") == False).count()\n\nplt.figure(figsize=(4, 2))  # Tamanho da KPI\nplt.text(0.5, 0.5, f'Pedidos em Aberto:\\n{pedidos_em_aberto}',\n         fontsize=14, ha='center', va='center', bbox=dict(facecolor='lightcoral', alpha=0.5))\n\nplt.axis('off')  # Remove os eixos\n\nplt.show()\n\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, year, month\nfrom datetime import datetime, timedelta\n\n\nspark = SparkSession.builder.appName(\"KPI_SomaTaxaCartaoCredito\").getOrCreate()\n\n\ndata_inicio = datetime.now().replace(day=1) - timedelta(days=1)\ndata_fim = datetime(data_inicio.year, data_inicio.month, 1)\n\n\nobt_ultimo_mes_cartao_credito = obt.filter((col(\"data_hora_pedido\") &gt;= data_fim)\n                                          &amp; (col(\"data_hora_pedido\") &lt;= data_inicio)\n                                          &amp; (col(\"tipo_pagamento\") == \"Cart\u00e3o de Cr\u00e9dito\"))\n\n\nsoma_taxa_cartao_credito = obt_ultimo_mes_cartao_credito.agg(sum(\"taxa_pagamento\").alias(\"soma_taxa\")).collect()[0][\"soma_taxa\"]\n\nplt.figure(figsize=(4, 2))  # Tamanho da KPI\nplt.text(0.5, 0.5, f'Taxa de recebimento\\nde Cart\u00e3o de Cr\u00e9dito:\\nR$ {soma_taxa_cartao_credito:.2f}',\n         fontsize=14, ha='center', va='center', bbox=dict(facecolor='lightblue', alpha=0.5))\n\nplt.axis('off')  # Remove os eixos\n\n\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom datetime import datetime, timedelta\n\nspark = SparkSession.builder.appName(\"FaturamentoUltimosSeteDias\").getOrCreate()\n\ndata_atual = datetime.now().date()\ndatas_para_exibir = [(data_atual.replace(day=23) - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(7)]\n\nfaturamento_ultimos_sete_dias = obt.filter(\n    F.col('data_hora_pedido').cast('date').isin(datas_para_exibir)\n).groupBy(\n    F.col('data_hora_pedido').cast('date').alias('data')\n).agg(\n    F.sum('valor_total').alias('faturamento_total')\n).orderBy(\n    'data'\n)\n\nfaturamento_ultimos_sete_dias_pd = faturamento_ultimos_sete_dias.toPandas()\n\nfaturamento_ultimos_sete_dias_pd['data'] = pd.to_datetime(faturamento_ultimos_sete_dias_pd['data'])\nfaturamento_ultimos_sete_dias_pd['data'] = faturamento_ultimos_sete_dias_pd['data'].dt.strftime('%d/%m/%Y')\n\nfaturamento_ultimos_sete_dias_pd = faturamento_ultimos_sete_dias_pd.sort_values(by='data')\n\nplt.figure(figsize=(10, 6))\nplt.bar(faturamento_ultimos_sete_dias_pd['data'], faturamento_ultimos_sete_dias_pd['faturamento_total'], color='green')\nplt.title('Faturamento Di\u00e1rio nos \u00daltimos 7 Dias')\nplt.xlabel('Data')\nplt.ylabel('Faturamento Total (R$)')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\ntotal_faturado = faturamento_ultimos_sete_dias_pd['faturamento_total'].sum()\nplt.annotate(f'Total faturado: R$ {total_faturado:.2f}',\n             xy=(0.5, 0.5),\n             xycoords='axes fraction',\n             xytext=(0, 20),\n             textcoords='offset points',\n             ha='center',\n             fontsize=12,\n             bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n\nplt.show()\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\nspark = SparkSession.builder.appName(\"KPIComandasComBebida\").getOrCreate()\n\ncomandas_com_bebida = obt.filter(F.col('categoria_item_cardapio') == 'Bebida')\n\ntotal_comandas = obt.select('codigo_comanda').distinct().count()\n\ntotal_comandas_com_bebida = comandas_com_bebida.select('codigo_comanda').distinct().count()\n\nporcentagem_comandas_com_bebida = (total_comandas_com_bebida / total_comandas) * 100\n\nimport matplotlib.pyplot as plt\n\nlabels = ['Comandas com Bebida E Comida', 'Comandas Individuais']\nsizes = [porcentagem_comandas_com_bebida, 100 - porcentagem_comandas_com_bebida]\ncolors = ['#ff9999','#66b3ff']\nexplode = (0.1, 0)  # explode 1st slice\n\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n        shadow=True, startangle=140)\n\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Porcentagem de Comandas com Bebida')\n\nplt.show()\n\nspark.read.format('delta').load(f'/mnt/{storageAccountName}/gold/obt_cafeteria').limit(10).display()\nobt.createOrReplaceTempView(\"obt_view\")\n%sql\nselect count(codigo_comanda) from obt_view\n</code></pre>"},{"location":"comoExecutar/#ir-para","title":"Ir para ...","text":"<ul> <li>-&gt; Apresenta\u00e7\u00e3o.</li> <li>-&gt; Pr\u00e9-Requisitos e Ferramentas.</li> <li>-&gt; Execu\u00e7\u00e3o.</li> </ul>"},{"location":"prerequisitos/","title":"Pre requisitos para executar o projeto","text":"<p>Segue os pre requisitos para execu\u00e7\u00e3o do projeto.</p> <ul> <li>Ser\u00e1 necess\u00e1rio uma conta na Microsoft Azure.</li> </ul>"},{"location":"prerequisitos/#criando-azure-data-lake-storage-no-azure","title":"Criando Azure Data Lake Storage no Azure","text":""},{"location":"prerequisitos/#disponibilizado-por-jlsilva01","title":"Disponibilizado por jlsilva01","text":""},{"location":"prerequisitos/#pre-requisitos","title":"Pr\u00e9-requisitos:","text":"<ul> <li>Azure CLI</li> <li>Visual Studio Code</li> <li>Terraform</li> <li>Conta Microsoft</li> </ul>"},{"location":"prerequisitos/#passos","title":"Passos:","text":"<p>1 - Efetuar o login no Azure atrav\u00e9s do Azure CLI.</p> <pre><code>az login\n</code></pre> <p>2 - Conferir sua assinatura atual.</p> <pre><code>az account show -o table\n</code></pre> <p>3 - Listar todas as assinaturas do Azure da sua conta Microsoft, utilizando o comando abaixo (troque o e-mail abaixo pelo e-mail da sua conta Azure).</p> <pre><code>az account list --query \"[?user.name=='&lt;Insira seu E-Mail Aqui&gt;'].{Name:name, ID:id, Default:isDefault}\" -o table\n</code></pre> <p>4 - Utilizar a sua assinatura do Azure (troque o \"Sua Assinatura\" abaixo pelo nome da sua assinatura da conta Azure).</p> <pre><code>az account set --subscription \"&lt;Sua Assinatura&gt;\"\n</code></pre> <p>5 - Consultar o nome do Resource Group criado para a sua conta.</p> <pre><code>az group list -o table\n</code></pre> <p>6 - Ajustar a vari\u00e1vel resource_group_name do arquivo <code>adls-azure-main/variables.tf</code> com o nome do Resource Group informado no passo anterior.</p> <pre><code>variable \"resource_group_name\" {\n  default = \"&lt;Seu Resource Group&gt;\"\n}\n</code></pre> <p>7 - Criar os recursos na assinatura Azure selecionada.</p> <pre><code>terraform init\n</code></pre> <pre><code>terraform validate\n</code></pre> <pre><code>terraform fmt\n</code></pre> <pre><code>terraform plan\n</code></pre> <pre><code>terraform apply\n</code></pre> <p>8 - Logar no portal.azure.com e conferir o deploy do ADLS.</p>"},{"location":"prerequisitos/#sql-server","title":"SQL Server","text":"<p>1 - Dentro do portal Azure, criar um banco SQL do Azure</p> <p>2 - Executar o script de cria\u00e7\u00e3o das tabelas no seu banco de dados. </p> <pre><code>CREATE TABLE \"comanda\"(\n    \"id_comanda\" INT NOT NULL,\n    \"id_mesa\" INT NOT NULL,\n    \"id_pagamento\" INT NOT NULL,\n    \"id_funcionario\" INT NOT NULL,\n    \"valor_total\" FLOAT NOT NULL\n);\nALTER TABLE\n    \"comanda\" ADD CONSTRAINT \"comanda_id_comanda_primary\" PRIMARY KEY(\"id_comanda\");\n\n\n\nCREATE TABLE \"pagamento\"(\n    \"id_pagamento\" INT NOT NULL,\n    \"tipo\" VARCHAR(25) NOT NULL,\n    \"status\" BIT NOT NULL,\n    \"taxa\" FLOAT NOT NULL\n);\nALTER TABLE\n    \"pagamento\" ADD CONSTRAINT \"pagamento_id_pagamento_primary\" PRIMARY KEY(\"id_pagamento\");\n\n\nCREATE TABLE \"ingredientes\"(\n    \"id_item_cardapio\" INT NOT NULL,\n    \"id_estoque\" INT NOT NULL,\n    \"quantidade\" INT NOT NULL,\n    \"unidade\" VARCHAR(10) NULL\n);\n\n\n\nCREATE TABLE \"estoque\"(\n    \"id_estoque\" INT NOT NULL,\n    \"ingrediente\" VARCHAR(30) NOT NULL,\n    \"quantidade\" FLOAT NOT NULL,\n    \"unidade\" VARCHAR(10) NULL\n);\nALTER TABLE\n    \"estoque\" ADD CONSTRAINT \"estoque_id_estoque_primary\" PRIMARY KEY(\"id_estoque\");\n\n\n\n\nCREATE TABLE \"funcionarios\"(\n    \"id_funcionario\" INT NOT NULL,\n    \"nome\" VARCHAR(20) NOT NULL,\n    \"sobrenome\" VARCHAR(50) NOT NULL,\n    \"telefone\" VARCHAR(30) NOT NULL,\n    \"email\" VARCHAR(50) NULL,\n    \"cargo\" VARCHAR(20) NOT NULL,\n    \"data_contratacao\" DATETIME NOT NULL\n);\nALTER TABLE\n    \"funcionarios\" ADD CONSTRAINT \"funcionarios_id_funcionario_primary\" PRIMARY KEY(\"id_funcionario\");\n\nCREATE TABLE \"cardapio\"(\n    \"id_item_cardapio\" INT NOT NULL,\n    \"nome_item\" VARCHAR(30) NOT NULL,\n    \"valor\" FLOAT NOT NULL,\n    \"descricao\" VARCHAR(80) NOT NULL,\n    \"categoria\" VARCHAR(30) NOT NULL,\n    \"disponibilidade\" BIT NOT NULL\n);\nALTER TABLE\n    \"cardapio\" ADD CONSTRAINT \"cardapio_id_item_cardapio_primary\" PRIMARY KEY(\"id_item_cardapio\");\nCREATE INDEX \"cardapio_nome_item_index\" ON\n    \"cardapio\"(\"nome_item\");\n\n\n\nCREATE TABLE \"mesas\"(\n    \"id_mesa\" INT NOT NULL,\n    \"qtd_lugares\" INT NOT NULL,\n    \"local\" VARCHAR(100) NOT NULL,\n    \"status\" BIT NOT NULL\n);\nALTER TABLE\n    \"mesas\" ADD CONSTRAINT \"mesas_id_mesa_primary\" PRIMARY KEY(\"id_mesa\");\n\n\n\nCREATE TABLE \"pedido\"(\n    \"id_pedido\" INT NOT NULL,\n    \"id_comanda\" INT NOT NULL,\n    \"id_cardapio\" INT NOT NULL,\n    \"status\" BIT NOT NULL,\n    \"data_hora_pedido\" DATETIME NOT NULL,\n    \"quantidade\" INT NOT NULL\n);\n\nALTER TABLE\n    \"pedido\" ADD CONSTRAINT \"pedido_id_pedido_primary\" PRIMARY KEY(\"id_pedido\");\n\nALTER TABLE\n    \"comanda\" ADD CONSTRAINT \"comanda_id_funcionario_foreign\" FOREIGN KEY(\"id_funcionario\") REFERENCES \"funcionarios\"(\"id_funcionario\");\nALTER TABLE\n    \"comanda\" ADD CONSTRAINT \"comanda_id_mesa_foreign\" FOREIGN KEY(\"id_mesa\") REFERENCES \"mesas\"(\"id_mesa\");\nALTER TABLE\n    \"comanda\" ADD CONSTRAINT \"comanda_id_pagamento_foreign\" FOREIGN KEY(\"id_pagamento\") REFERENCES \"pagamento\"(\"id_pagamento\");\nALTER TABLE\n    \"ingredientes\" ADD CONSTRAINT \"ingredientes_id_item_cardapio_foreign\" FOREIGN KEY(\"id_item_cardapio\") REFERENCES \"cardapio\"(\"id_item_cardapio\");\nALTER TABLE\n    \"ingredientes\" ADD CONSTRAINT \"ingredientes_id_estoque_foreign\" FOREIGN KEY(\"id_estoque\") REFERENCES \"estoque\"(\"id_estoque\");\nALTER TABLE\n    \"pedido\" ADD CONSTRAINT \"pedido_id_cardapio_foreign\" FOREIGN KEY(\"id_cardapio\") REFERENCES \"cardapio\"(\"id_item_cardapio\");\nALTER TABLE\n    \"pedido\" ADD CONSTRAINT \"pedido_id_comanda_foreign\" FOREIGN KEY(\"id_comanda\") REFERENCES \"comanda\"(\"id_comanda\");\n</code></pre> <p>3 - Executar o codigo <code>./PythonFaker/faker.ipynb</code> alterando as seguintes variaveis para as respectivas variaveis do seu banco. <pre><code># Definir os detalhes da conex\u00e3o\nserver = ''\ndatabase = ''\nusername = ''\npassword = ''\n</code></pre></p>"},{"location":"prerequisitos/#databricks","title":"DataBricks","text":"<p>1 - Dentro do Portal Azure, criar um recurso DataBricks.</p> <p>2 - Entre no Workspace Databricks, v\u00e1 na aba computa\u00e7\u00e3o e crie um Cluster.</p> <p>3 - Na aba espa\u00e7o de trabalhos, conecte o databricks com o reposit\u00f3rio no GitHub. </p> <p>4 - No Notebook <code>Notebooks Databricks/Landing/LakeHouse - Landing.py</code> Alterar as variaveis (Host, Database, Username, Password) para os dados do seu banco. <pre><code>Host = \"\"\nPort = 1433\nDatabase = \"\"\nUsername = \"\"\nPassword = \"\"\nDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nUrl = f\"jdbc:sqlserver://{Host}:{Port};databaseName={Database}\"\n</code></pre></p> <p>5- Em todos os Notebooks alterar as seguintes variaveis para os dados do seu Azure Data Lake storage. <pre><code>storageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken = \"\"\n</code></pre></p>"},{"location":"prerequisitos/#ir-para","title":"Ir para ...","text":"<ul> <li>-&gt; Apresenta\u00e7\u00e3o.</li> <li>-&gt; Pr\u00e9-Requisitos e Ferramentas.</li> <li>-&gt; Execu\u00e7\u00e3o.</li> </ul>"}]}